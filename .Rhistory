facet_wrap(~ Year)
# Now let's see what our results are if we change our tokens
inaug.words.five <- inaug.td %>%
tidytext::unnest_tokens(five_gram, text, token = "ngrams", n = 5) %>%
count(five_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(five_gram = reorder(five_gram, n)) %>%
ggplot(aes(five_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words.five
five_gram
# Now make this plot for bigrams. Hint: check ?unnest_tokens.
inaug.words.twoe <- inaug.td %>%
tidytext::unnest_tokens(two_gram, text, token = "ngrams", n = 2) %>%
count(two_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(two_gram = reorder(two_gram, n)) %>%
ggplot(aes(two_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words.two
# Now make this plot for bigrams. Hint: check ?unnest_tokens.
inaug.words.two <- inaug.td %>%
tidytext::unnest_tokens(two_gram, text, token = "ngrams", n = 2) %>%
count(two_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(two_gram = reorder(two_gram, n)) %>%
ggplot(aes(two_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words.two
### Clear terminal
cat("\014")
rt_combi = readRDS("twitter_data/rt_filt_censor.rds")
library(stringi) # For text manipulation
library(tidytext)
library(tm) # Framework for text mining
library(tidyr)
library(dplyr)
library(quanteda)
library(ggplot2)
#removing duplicates because, w/ no retweets, we will assume that large number of duplicates means bots
my_corpus <- rt_combi$text %>%
unique %>%
VectorSource %>%
VCorpus %>%
corpus
twitter.td <- tidy(my_corpus)
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) -> twitter.words
twitter.dfm <- dfm(my_corpus)
textplot_wordcloud(twitter.dfm)
### Clear terminal
cat("\014")
rt_combi = readRDS("twitter_data/rt_filt_censor.rds")
setwd("~/CompTextAnalysis/uvashortcourse")
### Clear terminal
cat("\014")
rt_combi = readRDS("twitter_data/rt_filt_censor.rds")
library(stringi) # For text manipulation
library(tidytext)
library(tm) # Framework for text mining
library(tidyr)
library(dplyr)
library(quanteda)
library(ggplot2)
#removing duplicates because, w/ no retweets, we will assume that large number of duplicates means bots
my_corpus <- rt_combi$text %>%
unique %>%
VectorSource %>%
VCorpus %>%
corpus
twitter.td <- tidy(my_corpus)
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) -> twitter.words
twitter.dfm <- dfm(my_corpus)
textplot_wordcloud(twitter.dfm)
rt_combi = readRDS("twitter_data/rt_shadow_ban.rds")
rt_combi = readRDS("twitter_data/rt_filt_shadow_ban.rds")
#removing duplicates because, w/ no retweets, we will assume that large number of duplicates means bots
my_corpus <- rt_combi$text %>%
unique %>%
VectorSource %>%
VCorpus %>%
corpus
twitter.td <- tidy(my_corpus)
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) -> twitter.words
twitter.dfm <- dfm(my_corpus)
textplot_wordcloud(twitter.dfm)
head(stop_words)
rt_combi = readRDS("twitter_data/rt_filt_censor.rds")
library(stm)
# cut down common words first
docs <- VCorpus(VectorSource(rt_combi$text))
# preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 100)))
custom.stop
custom.stop %>% head
custom.stop %>% typeof
rbind(custom.stop,"shadow")
typeof("this")
custom.stop %>% typeof
paste(custom.stop, "shadow")
c(custom.stop,"shadow")
stop_words
stop_words[[1,]]
stop_words[[,1]]
head(stop_words)
add_row(stop_words,word="shadow",lexicon=SMART)
add_row(stop_words,word="shadow",lexicon="SMART"")
add_row(stop_words,word="shadow",lexicon="SMART")
stop_words %>% tail
add_row(stop_words,word="ban",lexicon="SMART")
add_row(stop_words,word="ban",lexicon="SMART") %>% TAIL
add_row(stop_words,word="ban",lexicon="SMART") %>% tail
add_row(stop_words,word="ban",lexicon="custom") %>% tail
tail(stopwords)
tail(stop_words)
add_row(stop_words,word="ban",lexicon="custom") ->stop_words
add_row(stop_words,word="shadow",lexicon="custom") ->stop_words
# add search words to stop word list for word cloud
add_row(stop_words,word="ban",lexicon="custom") -> stop_words.custom
add_row(stop_words,word="shadow",lexicon="custom") -> stop_words.custom
add_row(stop_words,word="shadowban",lexicon="custom") -> stop_words.custom
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words.custom) -> twitter.words
twitter.dfm <- dfm(my_corpus)
textplot_wordcloud(twitter.dfm)
twitter.dfm
twitter.dfm %>% head
View(twitter.dfm)
twitter.dfm <- dfm(my_corpus, remove=stop_words.custom)
textplot_wordcloud(twitter.dfm)
stopwords("engish")
stopwords("english")
stopwords("english") %>% typeof
stop_words.custom
stop_words.custom$word
stop_words.custom$word %>% head
twitter.dfm <- dfm(my_corpus, remove=stop_words.custom$word)
textplot_wordcloud(twitter.dfm)
add_row(stop_words,word="t.co",lexicon="custom") -> stop_words.custom
add_row(stop_words,word="shadow",lexicon="custom") -> stop_words.custom
add_row(stop_words,word="shadowban",lexicon="custom") -> stop_words.custom
add_row(stop_words,word="https",lexicon="custom") -> stop_words.custom
add_row(stop_words,word="t.co",lexicon="custom") -> stop_words.custom
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words.custom) -> twitter.words
twitter.dfm <- dfm(my_corpus, remove=stop_words.custom$word)
textplot_wordcloud(twitter.dfm)
# add search words to stop word list for word cloud
add_row(stop_words,word="ban",lexicon="custom") -> stop_words.custom
add_row(stop_words,word="shadow",lexicon="custom") -> stop_words.custom
add_row(stop_words,word="shadowban",lexicon="custom") -> stop_words.custom
add_row(stop_words,word="https",lexicon="custom") -> stop_words.custom
add_row(stop_words,word="t.co",lexicon="custom") -> stop_words.custom
# add search words to stop word list for word cloud
add_row(stop_words,word="ban",lexicon="custom") -> stop_words.custom
add_row(stop_words.custom,word="shadow",lexicon="custom") -> stop_words.custom
add_row(stop_words.custom,word="shadowban",lexicon="custom") -> stop_words.custom
add_row(stop_words.custom,word="https",lexicon="custom") -> stop_words.custom
add_row(stop_words.custom,word="t.co",lexicon="custom") -> stop_words.custom
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words.custom) -> twitter.words
twitter.dfm <- dfm(my_corpus, remove=stop_words.custom$word)
textplot_wordcloud(twitter.dfm)
add_row(stop_words.custom,word="/.",lexicon="custom") -> stop_words.custom
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words.custom) -> twitter.words
twitter.dfm <- dfm(my_corpus, remove=stop_words.custom$word)
textplot_wordcloud(twitter.dfm)
# add search words to stop word list for word cloud
add_row(stop_words,word="ban",lexicon="custom") -> stop_words.custom
add_row(stop_words.custom,word="shadow",lexicon="custom") -> stop_words.custom
add_row(stop_words.custom,word="shadowban",lexicon="custom") -> stop_words.custom
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words.custom) -> twitter.words
twitter.dfm <- dfm(my_corpus, remove=stop_words.custom$word)
textplot_wordcloud(twitter.dfm)
kwic(twitter.dfm, pattern="american") %>%
textplot_xray()
kwic(my_corpus, pattern="american") %>%
textplot_xray()
rt_combi = readRDS("twitter_data/rt_filt_censor.rds")
#removing duplicates because, w/ no retweets, we will assume that large number of duplicates means bots
my_corpus <- rt_combi$text %>%
unique %>%
VectorSource %>%
VCorpus %>%
corpus
twitter.td <- tidy(my_corpus)
# add search words to stop word list for word cloud
add_row(stop_words,word="ban",lexicon="custom") -> stop_words.custom
add_row(stop_words.custom,word="shadow",lexicon="custom") -> stop_words.custom
add_row(stop_words.custom,word="shadowban",lexicon="custom") -> stop_words.custom
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words.custom) -> twitter.words
# add search words to stop word list for word cloud
add_row(stop_words,word="censor",lexicon="custom") -> stop_words.custom
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words.custom) -> twitter.words
twitter.dfm <- dfm(my_corpus, remove=stop_words.custom$word)
textplot_wordcloud(twitter.dfm)
add_row(stop_words.custom,word="censorship",lexicon="custom") -> stop_words.custom
twitter.td %>%
unnest_tokens(word, text) %>%
anti_join(stop_words.custom) -> twitter.words
twitter.dfm <- dfm(my_corpus, remove=stop_words.custom$word)
textplot_wordcloud(twitter.dfm, max_words=30)
textplot_wordcloud(twitter.dfm, max_words=75)
textplot_wordcloud(twitter.dfm, max_words=100)
kwic(my_corpus, pattern="@realdonaldtrump") %>%
textplot_xray()
kwic(my_corpus, pattern="realdonaldtrump") %>%
textplot_xray()
kwic(my_corpus, pattern="shadow") %>%
textplot_xray()
textplot_xray( # create multiple lexicon dispersion plots
kwic(my_corpus, pattern = "shadow"),
kwic(my_corpus, pattern = "ban"),
kwic(my_corpus, pattern = "censorship")
)
textplot_xray( # create multiple lexicon dispersion plots
kwic(my_corpus, pattern = "shadow"),
kwic(my_corpus, pattern = "censorship")
)
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
rt_combi = readRDS("twitter_data/rt_filt_censor.rds")
rt_combi = readRDS("twitter_data/rt_filt_censor.rds")
library(stm)
# cut down common words first
docs <- VCorpus(VectorSource(rt_combi$text))
# preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
# turn DTM into dataframe
dtm.m <- as.data.frame(as.matrix(dtm))
dtm.m$that <- NULL # fix weird stopword issue.
dim(dtm)
inspec(dtm[,100:104])
inspect(dtm[,100:104])
rownames(dtm.m)
dtm.m$shadowban
dtm.m.shadowban <- dt.m[dtm.m$shadowban == 1,]
dtm.m.shadowban <- dtm.m[dtm.m$shadowban == 1,]
head(dtm.m.shadowban)
head(dtm.m)
dtm.m.censorship <- dtm.m[dtm.m$censorship == 1,]
sum
sum(dtm.m.shadowban$aabort)
dtm.m.shadowban %>% colSums
dtm.m.shadowban %>% colSums -> shadow.sums
dtm.m.censorship %>% colSums ->censorship.sums
head(censorship.sums)
df <- data.frame(rbind(dtm.m.shadowban, dtm.m.censorship))
df
df[.1:5]
df[,1:5]
inspect(df)
View(df)
df[,dtm.m.shadowban > 0 & dmt.m.censorship > 0]
colSums(dtm.m.shadowban)
colSums(dtm.m.shadowban) -> s1
head(s1)
typeof(s1)
s1[[1]]
s1[[2]]
s1[[3]]
s1 %>% rownames
isGoodNumber3 <- function(X)
{ X[ ifelse(X!=0, TRUE,FALSE)]
}
Filter(s1, isGoodNumber3)
s1[s1>0]
s1[s1>0] ->s2
length(s1)
length(s2)
shadow.sums[shadow.sums > 0] -> shadow.sums
dtm.m.censorship %>% colSums ->censorship.sums
censorship.sums[censorship.sums > 0] -> censorship.sums
censorship.sums %>% LENGTH
censorship.sums %>% length
censorship.sums[censorship.sums > 0] %>%
sort(decreasing = FALSE) -> censorship.sums
censorship.sums[censorship.sums > 0] %>%
sort(decreasing = TRUE) -> censorship.sums
head(censorship.sums)
shadow.sums[shadow.sums > 0] %>%
sort(decreasing = TRUE) -> shadow.sums
shadow.sums %>% head
# eliminate search term from result
dtm.m$censorship <- NULL
dtm.m.shadowban <- dtm.m[dtm.m$shadowban == 1,]
dtm.m.censorship <- dtm.m[dtm.m$censorship == 1,]
dtm.m.shadowban %>% colSums -> shadow.sums
shadow.sums[shadow.sums > 0] %>%
sort(decreasing = TRUE) -> shadow.sums
dtm.m.censorship %>% colSums ->censorship.sums
censorship.sums[censorship.sums > 0] %>%
sort(decreasing = TRUE) -> censorship.sums
head(censorship.sums)
dtm.m.shadowban <- dtm.m[dtm.m$shadowban == 1,]
dtm.m.censorship <- dtm.m[dtm.m$censorship == 1,]
dtm.m.censorship
dtm.m.shadowban %>% colSums -> shadow.sums
shadow.sums %>% head
shadow.sums[shadow.sums > 0] %>%
sort(decreasing = TRUE) -> shadow.sums
shadow.sums %>% head
dtm.m.censorship %>% colSums ->censorship.sums
censorship.sums[censorship.sums > 0] %>%
sort(decreasing = TRUE) -> censorship.sums
head(censorship.sums)
dtm.m.censorship %>% colSums ->censorship.sums
head(censorship.sums)
dtm.m$that <- NULL # fix weird stopword issue.
dtm.m.shadowban <- dtm.m[dtm.m$shadowban == 1,]
dtm.m.censorship <- dtm.m[dtm.m$censorship == 1,]
dtm.m.shadowban %>% colSums -> shadow.sums
shadow.sums[shadow.sums > 0] %>%
sort(decreasing = TRUE) -> shadow.sums
dtm.m.censorship %>% colSums ->censorship.sums
censorship.sums[censorship.sums > 0] %>%
sort(decreasing = TRUE) -> censorship.sums
head(censorship.sums,10)
# turn DTM into dataframe
dtm.m <- as.data.frame(as.matrix(dtm))
dtm.m$that <- NULL # fix weird stopword issue.
dtm.m.shadowban <- dtm.m[dtm.m$shadowban == 1,]
dtm.m.censorship <- dtm.m[dtm.m$censorship == 1,]
dtm.m.shadowban %>% colSums -> shadow.sums
shadow.sums[shadow.sums > 0] %>%
sort(decreasing = TRUE) -> shadow.sums
dtm.m.censorship %>% colSums ->censorship.sums
censorship.sums[censorship.sums > 0] %>%
sort(decreasing = TRUE) -> censorship.sums
head(censorship.sums,10)
# turn DTM into dataframe
dtm.m <- as.data.frame(as.matrix(dtm))
dtm.m$that <- NULL # fix weird stopword issue.
dtm.m.shadowban <- dtm.m[dtm.m$shadowban == 1,]
dtm.m.censorship <- dtm.m[dtm.m$censorship == 1,]
dtm.m.shadowban %>% colSums -> shadow.sums
shadow.sums[shadow.sums > 0] %>%
sort(decreasing = TRUE) -> shadow.sums
dtm.m.censorship %>% colSums ->censorship.sums
censorship.sums[censorship.sums > 0] %>%
sort(decreasing = TRUE) -> censorship.sums
head(censorship.sums,10)
head(censorship.sums,2:10)
censorship.sums[,2:10]
censorship.sums[2:10]
censorship.sums[2:10] / 8846
censorship.sums[2:10] / censorship.sums[[1]]
cast_percentages <- function(X,n) {
X[2:n+1] / X[[1]]
}
dtm.m.censorship %>% cast_percentages(10)
censorship.sums %>% cast_percentages(10)
censorship.sums[2:10] / 8846
X[1:n+1] / X[[1]]
cast_percentages <- function(X,n) {
X[1:n+1] / X[[1]]
}
censorship.sums[2:10] / 8846
censorship.sums %>% cast_percentages(10)
shadow.sums %>% cast_percentages(10)
X[1:n+1] / X[[1]] %>%
round(digits=4)
cast_percentages <- function(X,n) {
X[1:n+1] / X[[1]] %>%
round(digits=4)
}
shadow.sums %>% cast_percentages(10)
X[1:n+1] / X[[1]] %>%
map(function(x) round(x, digits=4))
cast_percentages <- function(X,n) {
X[1:n+1] / X[[1]] %>%
map(function(x) round(x, digits=4))
}
shadow.sums %>% cast_percentages(10)
library(purr)
cast_percentages <- function(X,n) {
X[1:n+1] / X[[1]] %>%
map(function(x) round(x, digits=4))
}
shadow.sums %>% cast_percentages(10)
library(purrr)
cast_percentages <- function(X,n) {
X[1:n+1] / X[[1]] %>%
map(function(x) round(x, digits=4))
}
shadow.sums %>% cast_percentages(10)
X[1:n+1] / X[[1]]
cast_percentages <- function(X,n) {
X[1:n+1] / X[[1]]
}
shadow.sums %>% cast_percentages(10)
shadow.sums %>% cast_percentages(10) -> s1
shadow.sums %>% cast_percentages(10) -> s1
map(s1, function(x) round(x, digits=4))
cast_percentages <- function(X,n) {
X[1:n+1] / X[[1]] %>%
map(function(x) round(x, digits=4))
}
source('~/CompTextAnalysis/uvashortcourse/project4.R', echo=TRUE)
censorship.sums[censorship.sums > 0] %>%
sort(decreasing = TRUE) -> censorship.sums
df <- data.frame(rbind(dtm.m.shadowban, dtm.m.censorship))
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
rt_combi = readRDS("twitter_data/rt_filt_censor.rds")
rt_combi = readRDS("twitter_data/rt_filt_censor.rds")
library(stm)
# cut down common words first
docs <- VCorpus(VectorSource(rt_combi$text))
# preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
# turn DTM into dataframe
dtm.m <- as.data.frame(as.matrix(dtm))
dtm.m$that <- NULL # fix weird stopword issue.
dtm.m.shadowban <- dtm.m[dtm.m$shadowban == 1,]
dtm.m.censorship <- dtm.m[dtm.m$censorship == 1,]
dtm.m.shadowban %>% colSums -> shadow.sums
shadow.sums[shadow.sums > 0] %>%
sort(decreasing = TRUE) -> shadow.sums
dtm.m.censorship %>% colSums ->censorship.sums
censorship.sums[censorship.sums > 0] %>%
sort(decreasing = TRUE) -> censorship.sums
library(purrr)
cast_percentages <- function(X,n) {
X[1:n+1] / X[[1]] %>%
map(function(x) round(x, digits=4))
}
cast_percentages(censorship.sums)
cast_percentages(censorship.sums,10)
cast_percentages <- function(X,n) {
s1 = X[1:n+1] / X[[1]]
map(s1, function(x) round(x, digits=4))
}
cast_percentages(censorship.sums,10)
cast_percentages <- function(X,n) {
s1 = X[1:n+1] / X[[1]]
#map(s1, function(x) round(x, digits=4))
}
cast_percentages(censorship.sums,10)
#map(s1, function(x) round(x, digits=4))
s1
cast_percentages <- function(X,n) {
s1 = X[1:n+1] / X[[1]]
#map(s1, function(x) round(x, digits=4))
s1
}
cast_percentages(censorship.sums,10)
cast_percentages(censorship.sums,10) -> s1
s1[,2]
s1 %>% typeof
s1[1]
s1[1][1]
censorship.sums %>% cast_percentages(10)
shadow.sums %>% cast_percentages(10)
dtm.m.ban <- dtm.m[dtm.m$ban == 1,]
ban.sums[ban.sums > 0] %>%
sort(decreasing = TRUE) -> ban.sums
ban.sums %>% cast_percentages(10)
dtm.m.ban %>% colSums ->ban.sums
ban.sums[ban.sums > 0] %>%
sort(decreasing = TRUE) -> ban.sums
ban.sums %>% cast_percentages(10)
ban.sums
ban.sums %>% HEAD
ban.sums %>% head
