} else {
x[["next"]]
}
}
# =======================================================================
# my code now
rt_combi <- rt1
for (i in 1:20) {
np <- attr(rt_combi, 'next_page')[[1]]
rt_new <- search_tweets(search_query,
fromDate = fd,
toDate = td,
premium = premium_api("30day", env_name),
parse = TRUE, n = 100,
safedir = safedir,
include_rts = FALSE,
token=token,
'next'= np)
np_new <- get_next_page(rt_new)
rt_new <- tweets_with_users(rt_new)
rt_combi <- rbind(rt_combi, rt_new)
attr(rt_combi, 'next_page') <- np_new[[1]]
Sys.sleep(2)
}
## search query terms
search_query = "shadowban lang:en -filter:retweets"
np = attr(rt1, 'next_page')[[1]]
# this works, and extracts the next_page token
rt1 <- search_30day(
search_query,
n=100,
fromDate = fd,
toDate = td,
env = "dev",
parse = TRUE,
token = token
)
## search query terms
search_query = "shadowban lang:en -is:retweets"
# this works, and extracts the next_page token
rt1 <- search_30day(
search_query,
n=100,
fromDate = fd,
toDate = td,
env = "dev",
parse = TRUE,
token = token
)
## search query terms
search_query = "shadowban lang:en -is:retweet"
# this works, and extracts the next_page token
rt1 <- search_30day(
search_query,
n=100,
fromDate = fd,
toDate = td,
env = "dev",
parse = TRUE,
token = token
)
rt2 <- search_tweets(search_query,
n=18000,
parse = TRUE,
safedir = safedir,
include_rts = FALSE,
token=token)
rt2 <- search_tweets(search_query,
n=18000,
parse = TRUE,
safedir = safedir,
token=token)
rt_new <- search_tweets(search_query,
fromDate = fd,
toDate = td,
premium = premium_api("30day", env_name),
parse = TRUE, n = 100,
safedir = safedir,
token=token,
'next'= np)
## search query terms
search_query = "shadowban lang:en"
# this works, and extracts the next_page token
rt1 <- search_30day(
search_query,
n=100,
fromDate = fd,
toDate = td,
env = "dev",
parse = TRUE,
token = token
)
np = attr(rt1, 'next_page')[[1]]
# =======================================================================
# my code now
rt_combi <- rt1
for (i in 1:20) {
np <- attr(rt_combi, 'next_page')[[1]]
rt_new <- search_tweets(search_query,
fromDate = fd,
toDate = td,
premium = premium_api("30day", env_name),
parse = TRUE, n = 100,
safedir = safedir,
token=token,
'next'= np)
np_new <- get_next_page(rt_new)
rt_new <- tweets_with_users(rt_new)
rt_combi <- rbind(rt_combi, rt_new)
attr(rt_combi, 'next_page') <- np_new[[1]]
Sys.sleep(2)
}
View(rt2)
View(rt_combi)
typeof(rt2)
## search query terms
search_query = "social censorship lang:en"
rt.censor <- search_tweets(search_query,
n=18000,
parse = TRUE,
safedir = safedir,
token=token)
rt2[rt2$is_retweet == FALSE,]
rt2[rt2$is_retweet == FALSE,] -> rt_filt
saveRDS(rt_combi,paste(safedir,"/rt_filt.rds",sep=''))
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
rt_combi = readRDS("twitter_data/rt_filt.rds")
library(tidyr)
library(dplyr)
library(stringi) # For text manipulation
library(tm) # Framework for text mining
library(tidytext) # Sentiment dictionaries
library(ggplot2)
#removing duplicates because, w/ no retweets, we will assume that large number of duplicates means bots
my_corpus <- rt_combi$text %>%
unique %>%
VectorSource %>%
VCorpus
tcorp = tidy(my_corpus)
twitter.words <- tcorp %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(stop_words)
# Plot top 15 most frequently used words in each category
twitter.words %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
library(reshape2)
library(wordcloud)
twitter.words %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
twitter.words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 10)
# Now let's look at specific sentiment
twitter.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
# Bigrams
twitter.words.2 <- tcorp %>%
tidytext::unnest_tokens(two_gram, text, token = "ngrams", n = 2) %>%
count(two_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(five_gram = reorder(two_gram, n)) %>%
ggplot(aes(two_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
twitter.words.2
dtm <- DocumentTermMatrix(my_corpus,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 1000)))
for (i in 1:length(df.world)) {
df.world$text[i] <- removeWords(df.world$text[i], custom.stop)
}
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
rt_combi = readRDS("twitter_data/rt_filt.rds")
library(stm)
# cut down common words first
docs <- VCorpus(VectorSource(rt_combi$text))
# preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 1000)))
for (i in 1:length(rt_combi)) {
rt_combi$text[i] <- removeWords(rt_combi$text[i], custom.stop)
}
temp <- textProcessor(documents = rt_combi$text,
metadata = rt_combi,
customstopwords = custom.stop)
# pre-process
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
# estimate w/ 3 topics
model <- stm(docs, vocab, 3, data = meta, seed = 15)
# plot model
plot(model)
#top words
labelTopics(model)
#example?
findThoughts(model, texts = meta$text, n = 1, topics = 1)
# plot
plot(model, type = "perspectives", topics = c(2,3)) # Topics #1 and #10
## search query terms
search_query = "social media censorship lang:en"
rt.censor <- search_tweets(search_query,
n=18000,
parse = TRUE,
safedir = safedir,
token=token)
library(rtweet)
## authenticate via access token
token <- create_token(
app = "research_uva_law",
consumer_key = "fZtNt3Sw1o6wCJJLNVvjbTqXQ",
consumer_secret = "0989iZLbmgiB0r8YUYO7xFcj3OLNNjc3wW0yT6NUrn3ZueTrwr",
access_token = "26522197-DwVCEuz4IZDXU52K9WYbuhShvs7q8idtLKqyEZ0Hq",
access_secret = "IXpGYVMxPEJvjPBTQMIi2YBMY6GXAbczPBxzYTntRQdaP")
## search query terms
search_query = "social media censorship lang:en"
# from date
fd = "202001010000"
# to date
td = "202001270000"
# from my twttier developer settings
env_name <- "dev"
# where to save data; should create in working directory if doesn't exist
safedir = 'twitter_data'
rt.censor <- search_tweets(search_query,
n=18000,
parse = TRUE,
safedir = safedir,
token=token)
View(rt.censor)
# remove retweets
rt.censor[rt.censor$is_retweet == FALSE,] -> rt_filt
rt.censor
# remove retweets
rt.censor[rt.censor$is_retweet == FALSE,] -> rt_filt
rt_filt
saveRDS(rt_filt,paste(safedir,"/rt_filt_censor.rds",sep=''))
rt_combi = readRDS("twitter_data/rt_filsmcensor.rds")
rt_combi = readRDS("twitter_data/rt_filt_smcensor.rds")
library(stm)
# cut down common words first
docs <- VCorpus(VectorSource(rt_combi$text))
# preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 1000)))
for (i in 1:length(rt_combi)) {
rt_combi$text[i] <- removeWords(rt_combi$text[i], custom.stop)
}
temp <- textProcessor(documents = rt_combi$text,
metadata = rt_combi,
customstopwords = custom.stop)
# pre-process
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
# estimate w/ 3 topics
model <- stm(docs, vocab, 3, data = meta, seed = 15)
# plot model
plot(model)
#top words
labelTopics(model)
#example?
findThoughts(model, texts = meta$text, n = 1, topics = 1)
temp <- textProcessor(documents = rt_combi$text,
metadata = rt_combi)
# pre-process
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
# estimate w/ 3 topics
model <- stm(docs, vocab, 3, data = meta, seed = 15)
# plot model
plot(model)
#top words
labelTopics(model)
#example?
findThoughts(model, texts = meta$text, n = 1, topics = 1)
# plot
plot(model, type = "perspectives", topics = c(2,3)) # Topics #1 and #10
# estimate w/ 3 topics
model <- stm(docs, vocab, 5, data = meta, seed = 15)
# plot model
plot(model)
#top words
labelTopics(model)
#example?
findThoughts(model, texts = meta$text, n = 1, topics = 1)
# plot
plot(model, type = "perspectives", topics = c(5,3)) # Topics #1 and #10
# estimate w/ 4 topics
model <- stm(docs, vocab, 4, data = meta, seed = 15)
# plot model
plot(model)
# preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 20)))
for (i in 1:length(rt_combi)) {
rt_combi$text[i] <- removeWords(rt_combi$text[i], custom.stop)
}
temp <- textProcessor(documents = rt_combi$text,
metadata = rt_combi,
customstopwords = custom.stop)
# pre-process
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
# estimate w/ 3 topics
model <- stm(docs, vocab, 3, data = meta, seed = 15)
# estimate w/ 5 topics
model <- stm(docs, vocab, 5, data = meta, seed = 15)
# plot model
plot(model)
# estimate w/ 3 topics
model <- stm(docs, vocab, 3, data = meta, seed = 15)
# estimate w/ 4 topics
model <- stm(docs, vocab, 4, data = meta, seed = 15)
# plot model
plot(model)
# estimate w/ 3 topics
model <- stm(docs, vocab, 2, data = meta, seed = 15)
# plot model
plot(model)
custom.stop <- rownames(as.data.frame(head(v, 100)))
for (i in 1:length(rt_combi)) {
rt_combi$text[i] <- removeWords(rt_combi$text[i], custom.stop)
}
temp <- textProcessor(documents = rt_combi$text,
metadata = rt_combi,
customstopwords = custom.stop)
# pre-process
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
# estimate w/ 3 topics
model <- stm(docs, vocab, 3, data = meta, seed = 15)
# estimate w/ 5 topics
model <- stm(docs, vocab, 5, data = meta, seed = 15)
# estimate w/ 4 topics
model <- stm(docs, vocab, 4, data = meta, seed = 15)
# plot model
plot(model)
# estimate w/ 5 topics
model <- stm(docs, vocab, 6, data = meta, seed = 15)
# plot model
plot(model)
## search query terms
search_query = "censorship lang:en"
# from date
fd = "202001010000"
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
library(rtweet)
## authenticate via access token
token <- create_token(
app = "research_uva_law",
consumer_key = "fZtNt3Sw1o6wCJJLNVvjbTqXQ",
consumer_secret = "0989iZLbmgiB0r8YUYO7xFcj3OLNNjc3wW0yT6NUrn3ZueTrwr",
access_token = "26522197-DwVCEuz4IZDXU52K9WYbuhShvs7q8idtLKqyEZ0Hq",
access_secret = "IXpGYVMxPEJvjPBTQMIi2YBMY6GXAbczPBxzYTntRQdaP")
## search query terms
search_query = "censorship lang:en"
# from date
fd = "202001010000"
# to date
td = "202001270000"
# from my twttier developer settings
env_name <- "dev"
# where to save data; should create in working directory if doesn't exist
safedir = 'twitter_data'
rt.censor <- search_tweets(search_query,
n=50000,
parse = TRUE,
safedir = safedir,
token=token,
retryonratelimit = TRUE)
# remove retweets
rt.censor[rt.censor$is_retweet == FALSE,] -> rt_filt
saveRDS(rt_filt,paste(safedir,"/rt_filt_censor.rds",sep=''))
saveRDS(rt.censor,paste(safedir,"/rt_censor.rds",sep=''))
rt_combi = readRDS("twitter_data/rt_filt_censor.rds")
library(stm)
# cut down common words first
docs <- VCorpus(VectorSource(rt_combi$text))
# preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 100)))
for (i in 1:length(rt_combi)) {
rt_combi$text[i] <- removeWords(rt_combi$text[i], custom.stop)
}
temp <- textProcessor(documents = rt_combi$text,
metadata = rt_combi,
customstopwords = custom.stop)
# pre-process
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
# estimate w/ 5 topics
model <- stm(docs, vocab, 6, data = meta, seed = 15)
# plot model
plot(model)
#top words
labelTopics(model)
#example?
findThoughts(model, texts = meta$text, n = 1, topics = 1)
# plot
plot(model, type = "perspectives", topics = c(5,3)) # Topics #1 and #10
# plot
plot(model, type = "perspectives", topics = c(1,6)) # Topics #1 and #10
