top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
source('~/CompTextAnalysis/uvashortcourse/03class3.R')
### Clear terminal
cat("\014")
### Clear space
rm(list = ls())
### Load library packages
library(stringi) # For text manipulation
library(tm) # Framework for text mining
library(tidytext) # Sentiment dictionaries
library(quanteda)
library(tidyr)
library(dplyr)
library(ggplot2)
data("data_corpus_inaugural")
inaug.td <- tidy(data_corpus_inaugural) # tidy text the corpus
inaug.words <- inaug.td %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(stop_words)
# Look at the dataframe
inaug.words
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
library(textdata)
package.install("textdata")
install.packages("textdata")
library(textdata)
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Look at the dataframe
inaug.words
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
### Let's do a word cloud version of this
library(reshape2)
library(wordcloud)
inaug.words %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
inaug.words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 10)
# Now let's look at specific sentiment
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
# Now let's look at specific sentiment
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
# Now let's look at specific sentiment
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, Year, sort = TRUE) %>%
group_by(sentiment, Year) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip() +
facet_wrap(~ Year)
# Now let's see what our results are if we change our tokens
inaug.words.five <- inaug.td %>%
tidytext::unnest_tokens(five_gram, text, token = "ngrams", n = 5) %>%
count(five_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(five_gram = reorder(five_gram, n)) %>%
ggplot(aes(five_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 10)
# Now let's look at specific sentiment
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
# Now let's look at specific sentiment
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, Year, sort = TRUE) %>%
group_by(sentiment, Year) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip() +
facet_wrap(~ Year)
# Now let's see what our results are if we change our tokens
inaug.words.five <- inaug.td %>%
tidytext::unnest_tokens(five_gram, text, token = "ngrams", n = 5) %>%
count(five_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(five_gram = reorder(five_gram, n)) %>%
ggplot(aes(five_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words.five
five_gram
# Now make this plot for bigrams. Hint: check ?unnest_tokens.
inaug.words.twoe <- inaug.td %>%
tidytext::unnest_tokens(two_gram, text, token = "ngrams", n = 2) %>%
count(two_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(two_gram = reorder(two_gram, n)) %>%
ggplot(aes(two_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words.two
# Now make this plot for bigrams. Hint: check ?unnest_tokens.
inaug.words.two <- inaug.td %>%
tidytext::unnest_tokens(two_gram, text, token = "ngrams", n = 2) %>%
count(two_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(two_gram = reorder(two_gram, n)) %>%
ggplot(aes(two_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words.two
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
### Load library packages
library(stm)
# Load the data. Notice that we have the text of the articles in 'docs', along with some metadata in 'meta'.
load("materials2/course/world-constitutions.RData")
summary(df.world)
setwd("~/CompTextAnalysis/uvashortcourse")
# Load the data. Notice that we have the text of the articles in 'docs', along with some metadata in 'meta'.
load("materials2/course/world-constitutions.RData")
summary(df.world)
summary(nchar(df.world$text))
# Pre-process
temp <- textProcessor(documents = df.world$text, metadata = df.world)
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
View(docs)
summary(docs)
l
# We're now going to estimate a topic model with 15 topics by regressing topical prevalence on a year covariate.
model <- stm(docs, vocab, 8, prevalence = ~ year, data = meta, seed = 15)
summary(model)
# Plot model
plot(model)
### Hmm. A lot of really common words here. A lot of noise. Let's think about how to reduce that. One thing that we could do is remove the most common words in the corpus.
# create corpus
docs <- VCorpus(VectorSource(df.world$text))
### Hmm. A lot of really common words here. A lot of noise. Let's think about how to reduce that. One thing that we could do is remove the most common words in the corpus.
# create corpus
docs <- VCorpus(VectorSource(df.world$text))
### Hmm. A lot of really common words here. A lot of noise. Let's think about how to reduce that. One thing that we could do is remove the most common words in the corpus.
# create corpus
load(tm)
### Hmm. A lot of really common words here. A lot of noise. Let's think about how to reduce that. One thing that we could do is remove the most common words in the corpus.
# create corpus
library(tm)
docs <- tm::VCorpus(VectorSource(df.world$text))
docs
# preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 1000)))
view(custom.stop)
View(custom.stop)
View(m)
summary(m)
View(m)
for (i in 1:length(df.world)) {
df.world$text[i] <- removeWords(df.world$text[i], custom.stop)
}
temp <- textProcessor(documents = df.world$text,
metadata = df.world,
customstopwords = custom.stop)
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
model <- stm(docs, vocab, 8, prevalence = ~ year, data = meta, seed = 15)
plot(model)
R.version
docs <- tm::VCorpus(VectorSource(df.world$text))
docs
# preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 1000)))
for (i in 1:length(df.world)) {
df.world$text[i] <- removeWords(df.world$text[i], custom.stop)
}
temp <- textProcessor(documents = df.world$text,
metadata = df.world,
customstopwords = custom.stop)
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
model <- stm(docs, vocab, 8, prevalence = ~ year, data = meta, seed = 15)
plot(model)
preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 1000)))
for (i in 1:length(df.world)) {
df.world$text[i] <- removeWords(df.world$text[i], custom.stop)
}
temp <- textProcessor(documents = df.world$text,
metadata = df.world,
customstopwords = custom.stop)
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
model <- stm(docs, vocab, 8, prevalence = ~ year, data = meta, seed = 15)
plot(model)
preprocess and create DTM
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 1000)))
for (i in 1:length(df.world)) {
df.world$text[i] <- removeWords(df.world$text[i], custom.stop)
}
temp <- textProcessor(documents = df.world$text,
metadata = df.world,
customstopwords = custom.stop)
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
model <- stm(docs, vocab, 8, prevalence = ~ year, data = meta, seed = 15)
plot(model)
dtm <- DocumentTermMatrix(docs,
control = list(tolower = TRUE,
stopwords = TRUE,
removeNumbers = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = TRUE)
custom.stop <- rownames(as.data.frame(head(v, 1000)))
for (i in 1:length(df.world)) {
df.world$text[i] <- removeWords(df.world$text[i], custom.stop)
}
temp <- textProcessor(documents = df.world$text,
metadata = df.world,
customstopwords = custom.stop)
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents
# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
model <- stm(docs, vocab, 25, prevalence = ~ year, data = meta, seed = 15)
plot(model)
# Top Words
labelTopics(model)
# Example Docs
findThoughts(model, texts = meta$text, n = 1, topics = 1)
# Let's plot some aspects of the model
plot(model, type = "perspectives", topics = c(5, 7)) # Topics #1 and #10
plot(model, type = "hist")
# Let's plot some aspects of the model
plot(model, type = "perspectives", topics = c(5, 7)) # Topics #1 and #10
plot(model, type = "hist")
# Corpus Summary
plot.STM(model, type = "summary", main = "")
# Estimate Covariate Effects
prep <- estimateEffect(1:8 ~ year, model, meta = meta, uncertainty = "Global", documents=docs)
summary(prep)
plot(prep, covariate = "year", topics = 8)
# Estimate Covariate Effects
prep <- estimateEffect(1:25 ~ year, model, meta = meta, uncertainty = "Global", documents=docs)
plot(prep, covariate = "year", topics = 25)
plot(prep, "year", method = "continuous", topics = 1:25)
plot(prep, covariate = "year", topics = 25)
