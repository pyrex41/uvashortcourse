#Install packages that work with main libraries
install.packages(c("reshape", "MASS", "psych", "Rglpk", "XML"), dependencies=TRUE)
setwd("~/CompTextAnalysis/uvashortcourse")
### Clear terminal
cat("\014")
### Clear space
rm(list = ls())
### Load library packages
library(stringi) # For text manipulation
library(tm) # Framework for text mining
library(tidytext) # Sentiment dictionaries
library(quanteda)
library(tidyr)
library(dplyr)
data("data_corpus_inaugural")
inaug.td <- tidy(data_corpus_inaugural) # tidy text the corpus
inaug.words <- inaug.td %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(stop_words)
# Look at the dataframe
inaug.words
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
library(ggplot)
library(ggplot2)
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
source('~/CompTextAnalysis/uvashortcourse/03class3.R')
### Clear terminal
cat("\014")
### Clear space
rm(list = ls())
### Load library packages
library(stringi) # For text manipulation
library(tm) # Framework for text mining
library(tidytext) # Sentiment dictionaries
library(quanteda)
library(tidyr)
library(dplyr)
library(ggplot2)
data("data_corpus_inaugural")
inaug.td <- tidy(data_corpus_inaugural) # tidy text the corpus
inaug.words <- inaug.td %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(stop_words)
# Look at the dataframe
inaug.words
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
library(textdata)
package.install("textdata")
install.packages("textdata")
library(textdata)
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Look at the dataframe
inaug.words
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
inaug.words %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
### Let's do a word cloud version of this
library(reshape2)
library(wordcloud)
inaug.words %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
inaug.words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 10)
# Now let's look at specific sentiment
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
# Now let's look at specific sentiment
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
# Now let's look at specific sentiment
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, Year, sort = TRUE) %>%
group_by(sentiment, Year) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip() +
facet_wrap(~ Year)
# Now let's see what our results are if we change our tokens
inaug.words.five <- inaug.td %>%
tidytext::unnest_tokens(five_gram, text, token = "ngrams", n = 5) %>%
count(five_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(five_gram = reorder(five_gram, n)) %>%
ggplot(aes(five_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 10)
# Now let's look at specific sentiment
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
# Now let's look at specific sentiment
inaug.words %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, Year, sort = TRUE) %>%
group_by(sentiment, Year) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip() +
facet_wrap(~ Year)
# Now let's see what our results are if we change our tokens
inaug.words.five <- inaug.td %>%
tidytext::unnest_tokens(five_gram, text, token = "ngrams", n = 5) %>%
count(five_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(five_gram = reorder(five_gram, n)) %>%
ggplot(aes(five_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words.five
five_gram
# Now make this plot for bigrams. Hint: check ?unnest_tokens.
inaug.words.twoe <- inaug.td %>%
tidytext::unnest_tokens(two_gram, text, token = "ngrams", n = 2) %>%
count(two_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(two_gram = reorder(two_gram, n)) %>%
ggplot(aes(two_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words.two
# Now make this plot for bigrams. Hint: check ?unnest_tokens.
inaug.words.two <- inaug.td %>%
tidytext::unnest_tokens(two_gram, text, token = "ngrams", n = 2) %>%
count(two_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(two_gram = reorder(two_gram, n)) %>%
ggplot(aes(two_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
inaug.words.two
setwd("~/CompTextAnalysis/uvashortcourse")
install.packages("rtweet")
library(rtweet)
## authenticate via access token
token <- create_token(
app = "premium_app_cc",
consumer_key = "kDbhxt4YrSflQ8KRDQNI7wgkJ",
consumer_secret = "cKWjxM9KSKbNFf6y9KgblwPZm6FnOGm9goG5dFcWWgHejqrbZX",
access_token = "6219402-R9CkaB4LrJ6gKIAju7GLaXj1r2LR8TGX95770bv4kH",
access_secret = "IeSKlMzcCWzS2lNDvSUOyyTblvbojznnbHvKNuVgT7Iaw")
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
"#shadowban", n = 18000, include_rts = FALSE
)
rt %>%
ts_plot("3 hours") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #shadowban Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
"#shadowban", n = 18000, include_rts = FALSE
)
rt %>%
ts_plot("3 hours") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #shadowban Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
"#shadowban", n = 18000, include_rts = True
)
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
"#shadowban", n = 18000, include_rts = TRUE
)
rt %>%
ts_plot("3 hours") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #shadowban Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
rt %>%
ts_plot(tmls, "weeks") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #shadowban Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
rt %>%
ts_plot(rt, "weeks") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #shadowban Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
rt %>%
ts_plot("weeks") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #shadowban Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
rt %>%
ts_plot("3 hours") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #shadowban Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
rt %>%
ts_plot("1 hours") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #shadowban Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
rt %>%
ts_plot("1 day") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #shadowban Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
colnames(rt)
head(rt$created_at)
tail(rt$created_at)
library(readtext)
install.packages(c("readtext", "textclean"))
