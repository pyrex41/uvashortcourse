sum.perc[[1]]
as.data.frame(sum.perc)
as.data.frame(sum.perc) %>% hist
as.data.frame(sum.perc)$sum.perc %>% hist
library(ggplot2)
as.data.frame(sum.perc) -> df
gather(df, cols, value)
gather(df, cols, value) -> df.tidy
ggplot(df_tidy, aes(x = value)) +
geom_density(aes(color=cols))
gather(df, cols, value) -> df_tidy
ggplot(df_tidy, aes(x = value)) +
geom_density(aes(color=cols))
ggplot(df_tidy, aes(x = value, y=cols)) +
geom_point(aes(color=cols), size=3) +
scale_x_continuous(breaks = c(0,25,50,75,100,125))
ggplot(df_tidy, aes(x = cols, y=value)) +
geom_boxplot(aes(fill=cols))
generateHistogram  <- function(columnName) {
#I used library(ggplot2)
houseDFPlot <- ggplot(data=DF, aes(x=DF[columnName]))
#layering
houseDFPlot + geom_histogram()
}
generateHistogram(sum.perc)
DF = df.tidy
generateHistogram(sum.perc)
DF = df
generateHistogram(sum.perc)
DF
generateHistogram  <- function(columnName) {
#I used library(ggplot2)
houseDFPlot <- ggplot(data=DF, aes(x=DF[columnName]))
#layering
houseDFPlot + geom_histogram()
}
ggplot(df, aes(x=df[sum.perc]))
df
df$sum.perc
ggplot(df, aes(x=df$sum.perc))
ggplot(df, x=df$sum.perc)
ggplot(df, y=df$sum.perc)
sum.perc
sum.perc
sum.perc
sum.perc
sums[sums > 0] %>%
sort(decreasing = TRUE)  %>% map_percentages(10) -> sum.perc
sum.perc
sum.perc
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
library(rtweet)
## authenticate via access token
token <- create_token(
app = "research_uva_law",
consumer_key = "fZtNt3Sw1o6wCJJLNVvjbTqXQ",
consumer_secret = "0989iZLbmgiB0r8YUYO7xFcj3OLNNjc3wW0yT6NUrn3ZueTrwr",
access_token = "26522197-DwVCEuz4IZDXU52K9WYbuhShvs7q8idtLKqyEZ0Hq",
access_secret = "IXpGYVMxPEJvjPBTQMIi2YBMY6GXAbczPBxzYTntRQdaP")
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
# be sure to set working directory first
rt_combi = readRDS("censor_filt.rds")
library(stm)
library(tidyr)
library(dplyr)
library(stringi) # For text manipulation
library(tm) # Framework for text mining
library(tidytext) # Sentiment dictionaries
library(ggplot2)
# cut down common words first
docs <- VCorpus(VectorSource(rt_combi$text))
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
data1 = readRDS("shadow_ban_filt.rds")
data2 = readRDS("censor_filt.rds")
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
library(rtweet)
## authenticate via access token
## do not abuse lol; will need to redact if i ever start paying for access
token <- create_token(
app = "research_uva_law",
consumer_key = "fZtNt3Sw1o6wCJJLNVvjbTqXQ",
consumer_secret = "0989iZLbmgiB0r8YUYO7xFcj3OLNNjc3wW0yT6NUrn3ZueTrwr",
access_token = "26522197-DwVCEuz4IZDXU52K9WYbuhShvs7q8idtLKqyEZ0Hq",
access_secret = "IXpGYVMxPEJvjPBTQMIi2YBMY6GXAbczPBxzYTntRQdaP")
## search query terms
search_query = "shadow ban lang:en"
search_query2 = "censorship lang:en"
# from date
fd = "202001010000"
# to date
td = "202001270000"
# from my twttier developer settings
env_name <- "dev"
# where to save/backup data; should create in working directory if doesn't exist
safedir = 'twitter_data'
# these will need to run for a while, since every 18k tweets they have to wait for 15min
search_query %>%
search_tweets(
n=50000,
parse = TRUE,
safedir = safedir,
token=token,
retryonratelimit = TRUE) -> res
View(res)
## search query terms
search_query = "shadowban lang:en"
# these will need to run for a while, since every 18k tweets they have to wait for 15min
search_query %>%
search_tweets(
n=50000,
parse = TRUE,
safedir = safedir,
token=token,
retryonratelimit = TRUE) -> res
View(res)
data1 = readRDS("shadowban_filt.rds")
library(tidyr)
library(dplyr)
library(stringi) # For text manipulation
library(tm) # Framework for text mining
library(tidytext) # Sentiment dictionaries
library(ggplot2)
#removing duplicates because, w/ no retweets, we will assume that large number of duplicates means bots
data1$text %>%
unique %>%
VectorSource %>%
VCorpus %>%
tidy -> my_corpus1
my_corpus1 %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(stop_words) -> twitter.words1
# Plot top 15 most frequently used words in each category
twitter.words1 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
twitter.words2 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 30) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
twitter.words1 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 30) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
twitter.words1 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 10) +
labs(y = NULL, x = NULL) +
coord_flip()
library(reshape2)
library(wordcloud)
twitter.words1 %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
twitter.words1 %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red2", "green3"),
max.words = 30)
# Now let's look at specific sentiment
twitter.words1 %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
# Now let's look at specific sentiment
twitter.words1 %>%
inner_join(get_sentiments("nrc"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(pos_neg = ifelse(sentiment %in% c("positive", "anticipation", "joy", "trust", "surprise"),
"Positive", "Negative")) %>%
ggplot(aes(reorder(sentiment, n), n)) +
geom_col(aes(fill = pos_neg), show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
# Bigrams
my_corpus1 %>%
tidytext::unnest_tokens(two_gram, text, token = "ngrams", n = 2) %>%
count(two_gram, sort = TRUE) %>%
top_n(20) %>%
mutate(two_gram = reorder(two_gram, n)) %>%
ggplot(aes(two_gram, n)) +
geom_col(fill = "red", show.legend = FALSE) +
xlab(NULL) +
ylab(NULL) +
coord_flip()
View(data1)
View(my_corpus1)
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
data1 = readRDS("shadowban_filt.rds")
View(data1)
data1 = readRDS("test.rds")
View(data1)
data1 = readRDS("test.rds")
data1 = readRDS("test.rds")
### Clear space
rm(list = ls())
### Clear terminal
cat("\014")
data2 = readRDS("censor_filt.rds")
library(tidyr)
library(dplyr)
library(stringi) # For text manipulation
library(tm) # Framework for text mining
library(tidytext) # Sentiment dictionaries
library(ggplot2)
#removing duplicates because, w/ no retweets, we will assume that large number of duplicates means bots
data1$text %>%
unique %>%
VectorSource %>%
VCorpus %>%
tidy -> my_corpus1
data1 = readRDS("censor_filt.rds")
#removing duplicates because, w/ no retweets, we will assume that large number of duplicates means bots
data1$text %>%
unique %>%
VectorSource %>%
VCorpus %>%
tidy -> my_corpus1
my_corpus1 %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(stop_words) -> twitter.words1
# Plot top 15 most frequently used words in each category
twitter.words1 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 10) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
twitter.words1 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 300) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
twitter.words1 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 500) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
twitter.words1 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 800) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
twitter.words1 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 600) +
labs(y = NULL, x = NULL) +
coord_flip()
add_row(stop_words, word='censorship', lexicon='custom') -> custom_stop_words
# Plot top 15 most frequently used words in each category
my_corpus1 %>%
unnest_tokens(word, text) %>%
anti_join(custom_stop_words) %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 600) +
labs(y = NULL, x = NULL) +
coord_flip()
# Plot top 15 most frequently used words in each category
twitter.words1 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 600) +
labs(y = NULL, x = NULL) +
coord_flip()
#custom stop
add_row(stop_words, word='censorship', lexicon='custom') -> custom_stop_words
my_corpus_sans %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(custom_stop_words) -> twitter.words.sans
my_corpus1 %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(custom_stop_words) -> twitter.words.sans
# word cloud
twitter.words.sans %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
add_row(stop_words, word='t.co', lexicon='custom') -> custom_stop_words
add_row(stop_words, word='https', lexicon='custom') -> custom_stop_words
add_row(stop_words, word='t.co', lexicon='custom') -> custom_stop_words
# word cloud
twitter.words.sans %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
my_corpus1 %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(custom_stop_words) -> twitter.words.sans
# word cloud
twitter.words.sans %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
#custom stop
add_row(stop_words, word='censorship', lexicon='custom') -> custom_stop_words
add_row(stop_words, word='https', lexicon='custom') -> custom_stop_words
add_row(stop_words, word='t.co', lexicon='custom') -> custom_stop_words
my_corpus1 %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(custom_stop_words) -> twitter.words.sans
my_corpus1 %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(stop_words) -> twitter.words1
# Plot top 15 most frequently used words in each category
twitter.words1 %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup() %>%
group_by(sentiment) %>%
top_n(15) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_manual(values = c("red2", "green3")) +
facet_wrap(~sentiment, scales = "free_y") +
ylim(0, 600) +
labs(y = NULL, x = NULL) +
coord_flip()
library(reshape2)
library(wordcloud)
# word cloud
twitter.words.sans %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
# word cloud
twitter.words.sans %>%
anti_join(custom_stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
# word cloud
twitter.words.sans %>%
anti_join(custom_stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
#custom stop
custom_stop_words <- stop_words
add_row(custom_stop_words, word='censorship', lexicon='custom') -> custom_stop_words
add_row(custom_stop_words, word='https', lexicon='custom') -> custom_stop_words
add_row(custom_stop_words, word='t.co', lexicon='custom') -> custom_stop_words
my_corpus1 %>% # convert the corpus to document-word
unnest_tokens(word, text) %>%
anti_join(custom_stop_words) -> twitter.words.sans
# word cloud
twitter.words.sans %>%
anti_join(custom_stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
# more funner word cloud
twitter.words1 %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red2", "green3"),
max.words = 30)
# more funner word cloud
twitter.words1 %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red2", "green3"),
max.words = 100)
